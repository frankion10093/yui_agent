# llm_config.yaml
llm:
  chat:
    ollama:
      # ollama访问地址
      base_url: http://192.168.31.100:11434
      # llm模型
      model: gpt-oss:20b
      # 模型的温度(0-1)数字越大随机性越强
      temperature: 0.6
      # 保持模型在内存的时间
      keep_alive: 3600
      # 请求超时时间
      timeout: 60
    siliconflow:
      # siliconflow访问地址
      base_url: https://api.siliconflow.cn/v1
      # 模型的api_key
      api_key: sk-apiakdsksjonkqeeilvubghnsqholuwhvdpfpccesjrirprd
      # llm大语言模型
      model: deepseek-ai/DeepSeek-V3.2
      # 模型的温度(0-1)数字越大随机性越强
      temperature: 0.6
      # 请求超时时间
      timeout: 60

  vl:
    siliconflow:
      base_url: https://api.siliconflow.cn/v1
      model: Qwen/Qwen3-VL-32B-Instruct
      api_key: sk-apiakdsksjonkqeeilvubghnsqholuwhvdpfpccesjrirprd
      temperature: 0.7
      timeout: 60

  
